* Day 1
- Learnt about sampling rate. Can't continuously sample because that would mean infinite information. We store the pressure variations. There is this Nyquist frequency, at which we should sample. This is twice the max frequency in the audio sample. Then from an ultimate theorem, called Shanon sampling theorem, we can find the whole signal.

- Why we go from time domain to Frequency? In time we can find when there is silence or not. For phonemes, which are fundamental speech units, there is a particular spectral envelope and a certain fine spectral structure. Doing Fourier Transform will help us find the required frequencies. Now we can define how many frequencies we want, the more we have the better it is. We keep it as the next power of 2, of our original frame size.

- While taking FFT, we use a hamming window, to nullify the effects of
  chosing the window, and to make the FFT feel like it is acting on
  continuous function. Hamming window, which is a period of cosine wave.

- For finding the MFCC's: We'll take the absolute value, square it and apply
  Mel filters. Number of filters used are 40. The question is why 40? So
  we'll vary this number while finding MFCC's. We take the logarithm to
  convert the large scale, or in a better way, the large range, to a smaller
  one. Hearing is based on a non linear [Mel] scale.Some DCT is applied for some reason. Do find that out.

* Day 2
- Learnt how mono and stereo files are stored. Lets say 8 bit mono. Then
  after 8 bits, new frame starts. Now, say we have stereo of 8 bits. Then in
  a 16 bit block, first 8 bit will be of one channel, and the other 8 bit of
  second channel. And this 16 bit will constitute one frame.
- Steps for Mel: First find the power spectrum, apply filters, take the log
  spectrum, then apply discrete cosine transform.
- Taking filters and then we take the weighted mean. We need to find why
  Discrete Cosine Transform. 

* Day 4
- Read about Neural Networks: Architecture, Need, Hidden Units, Output Units
- What kind of functions are required for Hidden and output units
- Used HTK tools: Specifically HCopy to generate MFCC and Delta And
  Acceleration features.
- Now, we actually need to find the frames where there is actual voice
  activity. That is, we perfrom VAD. This is done is using rVAD, which is a
  MAtlab based code. It requires as input a list, with each row having an
  input wav file, output pitch file, and output VAD file. The structure of
  VAD file is: Frame start, Frame End
- BONUS: Map VAD FRames, by multiplying with hop window, onto the original
  WAV files, and see how well VAD actually does, by changing the VAD thres.
  The smaller VAD thres, the more Noise should we be getting.
- Apply VAD Wrapper, to only get the features for the frames that match VAD
  extraction. For this, Give a list containing, input feature file, vad file
  and output feature file[keep it same, maybe?]
* Day 5
- Applying wrapper to use the VAD data on htk files. Essentially getting the
  features for only the frames that contain voice activity.
- Possible features: MFCC, Kurtosis, Spectral Flatness Measure, Context. I'm
  just using MFCC as a beginner stuff.

* Day 6
- Learnt about neaural nets, essentially the back propogation algorithm which
  uses the derivatives found in higher layers, to find the derivatives for
  lower layers.

* May 16:
- Equal error rate: In any biometric system, we would want to minimize the
  number of false positives and the false negatives that we get. This is the
  most used error rate, in ASR and stuff.
* May 17-18:
- Read about Convolutional Neural Nets. The core idea being in 
* May 19:
- Got to know about spectral flatness measure. This essentially measures how
  noise like, or how tone like, is the signal. We take the GM of power
  spectrum divided by AM of power spectrum. If the signal is noise like, that
  is a lot of frequencies have the same power spectrum, the SFM value will
  tend to be closer to one.
- Also, trained simple DNN models based on MFCC and Kurtosis. Using Kurtosis
  only marginally improves the performance.

* May 21:   
- Learnt about spectral flatness value and why it makes sense to find the sfm
  with log values, as while taking the geometric mean, the gm falls because
  of the chracteristic of Spectral Flatness is such that most of the values
  decrease. Thus if we take log, it will fall less, and thus sfm can be more
  useful as a feature.
- Also, white noise, ideally should have an SFM of 1, though in practice it
  turns out to be close to 0.6-0.7

